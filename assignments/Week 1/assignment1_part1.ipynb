{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "262ee4f1cc793d5cc0302d24cbc64461",
     "grade": false,
     "grade_id": "cell-aa820d6aaf4304db",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "version = \"REPLACE_PACKAGE_VERSION\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "288260708ed53d23fbee2b1333a77e8a",
     "grade": false,
     "grade_id": "cell-24e63ee011a83003",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "# Assignment 1 Part 1: N-gram Language Models (40 pts)\n",
    "\n",
    "In this assignment, we're going to train an n-gram language model that is able to \"imitate\" William Shakespeare's writing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d42c6f82b52e85920cee3cca49b7239a",
     "grade": false,
     "grade_id": "cell-0153dc3ed86e1f61",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Configure nltk\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk_data_path = \"assets/nltk_data\"\n",
    "if nltk_data_path not in nltk.data.path:\n",
    "    nltk.data.path.append(nltk_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d11c9af18458f110af7afffe909f9b86",
     "grade": false,
     "grade_id": "cell-d63bebc6fef0f0d1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 1: Load the dataset (10 pts)\n",
    "\n",
    "As the first step towards imitating Shakespeare's writing, you will create a function called `load_data` that loads his original *Sonnets* from `assets/gutenberg/THE_SONNETS.txt`. This function should accomplish the following: \n",
    "\n",
    "* **Extract sentences from the data file.** Of course, depending on the nature of the task at hand, what constitutes a *sentence* can vary. In the context of this assignment, we will define a sentence as just a line of a sonnet, regardless of the punctuation at end. In addition, we will ignore the boundaries of the sonnets --- that is, we are not dealing with $154$ individual *sonnets* but rather $154 \\times 14 = 2156$ *sentences* (actually only $2155$ sentences, as *Sonnet 99* has [15 lines](https://www.google.com/search?ei=po4hX9jzN4rKtQaL7K-wDg&q=why+is+sonnet+99+15+lines&oq=why+is+sonnet+99+15+lines&gs_lcp=CgZwc3ktYWIQAzoECAAQR1ChGlihGmCqHGgAcAJ4AIABXIgBXJIBATGYAQCgAQGqAQdnd3Mtd2l6wAEB&sclient=psy-ab&ved=0ahUKEwjY3snX3PLqAhUKZc0KHQv2C-YQ4dUDCAw&uact=5) but *Sonnet 126* has only [12](https://www.google.com/search?ei=d4whX6_uH9a4tQbUiISoDA&q=why+is+sonnet+126+only+12+lines&oq=o+thou+my+lovely+boy+who+in+thy+power&gs_lcp=CgZwc3ktYWIQARgBMgQIABBHMgQIABBHMgQIABBHMgQIABBHMgQIABBHMgQIABBHMgQIABBHMgQIABBHUABYAGCWEWgAcAJ4AIABAIgBAJIBAJgBAKoBB2d3cy13aXrAAQE&sclient=psy-ab)). We encourage you to explore alternative definitions of a sentence on your own; for example, an entire sonnet could be modelled as a sentence. Finally, make sure that the newline character `\\n` at the end of each line is dropped. \n",
    "\n",
    "\n",
    "* **Tokenise each extracted sentence.** While it's ambiguous what a sentence is, what constitutes a \"*word*\" is even more task-dependent. Do punctuations count as \"words\"? Are two \"words\" with the same spelling but different casing considered identical? Since what a text file contains is nothing more than a squence of characters, there are many possible ways of grouping these characters to form \"words\" that are subsequently taken as input by a program. To distinguish what's actually taken as input by a program from a linguistic word, we call the former a *token*. The process of producing a list of tokens out of a sentence is then called *tokenisation*. At this step, you will first lower-case each sentence extracted from the previous step entirely and then tokenise each lower-cased sentence. You may use any tokeniser of your choice, such as `word_tokenize` from the `nltk` library. The grading of the assignment doesn't depend on your choice of the tokeniser. \n",
    "\n",
    "**This function should return a `list` of length 2155, where each element is a `list` of `str` representing the tokens of each sentence as produced by the tokeniser of your choice. An example output would be:**\n",
    "\n",
    "```\n",
    "[['from', 'fairest', 'creatures', 'we', 'desire', 'increase', ','],\n",
    " ['that', 'thereby', 'beauty', '’', 's', 'rose', 'might', 'never', 'die', ','],\n",
    " ...\n",
    " ['came', 'there', 'for', 'cure', 'and', 'this', 'by', 'that', 'i', 'prove', ','], \n",
    " ['love', '’', 's', 'fire', 'heats', 'water', ',', 'water', 'cools', 'not', 'love', '.']]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d4ea2201f33d2455eb83f6c7c68e483",
     "grade": false,
     "grade_id": "cell-876ee5c9fdc4f347",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load text data from a file and produce a list of token lists\n",
    "    \"\"\"\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3e4184bbeb9f594851ab27528c1c385e",
     "grade": true,
     "grade_id": "cell-436f8cfe2405ffe8",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Autograder test\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m stu_ans \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stu_ans, \u001b[38;5;28mlist\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQ1: Your function should return a list. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(stu_ans) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2155\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQ1: There should be 2155 sentences. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m sentences \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# YOUR CODE HERE\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sentences\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Autograder test\n",
    "\n",
    "stu_ans = load_data()\n",
    "\n",
    "assert isinstance(stu_ans, list), \"Q1: Your function should return a list. \"\n",
    "assert len(stu_ans) == 2155, \"Q1: There should be 2155 sentences. \"\n",
    "\n",
    "for index, tokens in enumerate(stu_ans):\n",
    "    assert isinstance(tokens, list), f\"Q1: The element at index {index} of your answer list should be a list. \"\n",
    "    for token in tokens:\n",
    "        if token.isalpha():\n",
    "            assert token.islower(), f'Q1: Token \"{token}\" in the sentence at index {index} is not lower-cased. '\n",
    "        \n",
    "        assert token != \"\\n\", f'Q1: You should drop the \"\\\\n\" character in the sentence at index {index}. '\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "62ca73819ccfb33c161ca8f1298de083",
     "grade": false,
     "grade_id": "cell-902e515e7633ecbd",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 2: Build vocabulary (15 pts)\n",
    "\n",
    "Next, we need a \"vocabulary\" that contains all the unique tokens. Moreover, as mentioned in the lecture, we often pad a sentence with `<s>` and `</s>` to indicate its start and end when working with n-gram language models; therefore, these two special tokens should also be included in our vocabulary. Complete the function below to build a vocabulary. The order in which the tokens are stored doesn't matter. \n",
    "\n",
    "**This function should return a `list` of unique tokens, including `<s>` and `</s>`. An example output would be:**\n",
    "\n",
    "```\n",
    "['refuse', 'enjoyed', ..., '<s>', '</s>']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "19516439848342b7eedc50ef78c20fc7",
     "grade": false,
     "grade_id": "cell-be2a75b95ae4c066",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def build_vocab(sentences):\n",
    "    \"\"\"\n",
    "    Take a list of sentences and return a vocab\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab = []\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1b9d2caf50e94b70e0c65ad9b18e3818",
     "grade": true,
     "grade_id": "cell-bd592f77659516e2",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder tests\n",
    "\n",
    "stu_sents = load_data()\n",
    "stu_vocab = build_vocab(stu_sents)\n",
    "\n",
    "assert isinstance(stu_vocab, list), \"Q2: Your function should return a list. \"\n",
    "assert stu_vocab, \"Q2: Your vocab is empty. \"\n",
    "assert \"<s>\" in stu_vocab, \"Q2: Remember to include special token <s>. \"\n",
    "assert \"</s>\" in stu_vocab, \"Q2: Remember to include special token </s>. \"\n",
    "assert len(set(stu_vocab)) == len(stu_vocab), \"Q2: Your vocab contains duplicated tokens. \"\n",
    "\n",
    "# Some hidden tests\n",
    "\n",
    "del stu_sents, stu_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d7c6f4ee7affc1ab88ddd68db23652d7",
     "grade": false,
     "grade_id": "cell-4b22a356806c732d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 3: Generate all $n$-grams (15 pts)\n",
    "\n",
    "Now let's write a function to generate all $n$-grams for each sentence. This can be accomplished in two steps:\n",
    "* **Pad each sentence with `<s>` and `</s>` for $n \\geq 2$**. You need $n - 1$ paddings on both ends of a sentence, so that there are two $n$-grams that model the first and the last token, respectively. You may implement the padding function yourself or use the `pad_both_ends` function from the `nltk` library. \n",
    "\n",
    "\n",
    "* **Generate $n$-grams on the padded sentences**. Check out the `ngrams` function from `nltk`. For a sentence with $\\ell$ tokens excluding paddings, the maximum number of possible $n$-grams generated from its padded version should be $\\ell + n - 1$. Think about why. \n",
    "\n",
    "**Complete the function below to return a `list`, where each element of the `list` is a either a list or a \"generator object\" produced by the `ngrams` function, representing a sequence of all $n$-grams generated from each appropriately padded sentence. If the argument `n=2`, the autograder would accept either of the example outputs below:**\n",
    "\n",
    "```\n",
    "[<generator object ngrams at 0x7f77ed778b10>,\n",
    " <generator object ngrams at 0x7f77e7d934f8>,\n",
    " ...\n",
    " <generator object ngrams at 0x7f77e7d751b0>,\n",
    " <generator object ngrams at 0x7f77e7d75228>]\n",
    "```\n",
    "\n",
    "**OR**\n",
    "\n",
    "```\n",
    "[\n",
    "    [('<s>', 'from'), ('from', 'fairest'), ('fairest', 'creatures'), ('creatures', 'we'), ('we', 'desire'),\n",
    "     ('desire', 'increase'), ('increase', ','), (',', '</s>')], \n",
    "    \n",
    "    [('<s>', 'that'), ('that', 'thereby'), ('thereby', 'beauty'), ('beauty', '’'), ('’', 's'), ('s', 'rose'), \n",
    "     ('rose', 'might'), ('might', 'never'), ('never', 'die'), ('die', ','), (',', '</s>')],\n",
    "     \n",
    "    ...\n",
    "    \n",
    "    [('<s>', 'came'), ('came', 'there'), ('there', 'for'), ('for', 'cure'), ('cure', 'and'), ('and', 'this'), \n",
    "     ('this', 'by'), ('by', 'that'), ('that', 'i'), ('i', 'prove'), ('prove', ','), (',', '</s>')], \n",
    "    \n",
    "    [('<s>', 'love'), ('love', '’'), ('’', 's'), ('s', 'fire'), ('fire', 'heats'), ('heats', 'water'), \n",
    "     ('water', ','), (',', 'water'), ('water', 'cools'), ('cools', 'not'), ('not', 'love'), ('love', '.'), \n",
    "     ('.', '</s>')]\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d31c334494f0dfdc68a9918c9edf925",
     "grade": false,
     "grade_id": "cell-79211162e032488b",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def build_ngrams(n, sentences):\n",
    "    \"\"\"\n",
    "    Take a list of unpadded sentences and create all n-grams as specified by the argument \"n\" for each sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    all_ngrams = []\n",
    "    #HINT: use pad_both_ends from nltk library   \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return all_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "39aa98a54d9d0e6bee01e17de33dfe80",
     "grade": true,
     "grade_id": "cell-66bf33f44f8b83bf",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder tests\n",
    "\n",
    "stu_n = 4\n",
    "stu_sents = load_data()\n",
    "old_hash = hash(tuple([tuple(sent) for sent in stu_sents]))\n",
    "stu_ngrams = build_ngrams(stu_n, stu_sents)\n",
    "\n",
    "assert isinstance(stu_ngrams, list), \"Q3: Your function should return a list. \"\n",
    "assert stu_ngrams, \"Q3: Your ngrams list is empty. \"\n",
    "\n",
    "# Check that your function does not modify 'stu_sents' in place\n",
    "new_hash = hash(tuple([tuple(sent) for sent in stu_sents]))\n",
    "assert new_hash == old_hash, \"Q3: Your function should not modify its argument 'sentences' in place. \"\n",
    "\n",
    "# Some hidden tests\n",
    "\n",
    "\n",
    "del stu_n, stu_sents, stu_ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ad086d5cfb862a0e623e123e8fe7e500",
     "grade": false,
     "grade_id": "cell-caf5a40a85230775",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now that we have completed all the preparation work for imitating William Shakespeare's writing, it's time to take a break. We will resume in Assignment 1 Part 2 to finish training an $n$-gram language model. See you there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
