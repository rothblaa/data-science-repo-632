{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "262ee4f1cc793d5cc0302d24cbc64461",
     "grade": false,
     "grade_id": "cell-aa820d6aaf4304db",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "version = \"REPLACE_PACKAGE_VERSION\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "633eed63abf7c048ce331d6dd314c93d",
     "grade": false,
     "grade_id": "cell-24e63ee011a83003",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "# Assignment 4 Part 2: Counting in a Data Stream (50 pts)\n",
    "\n",
    "In this assignment, we're going to implement two algorithms for counting items in a data stream. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "70b42d4698cc4de05f7858fcd2719e15",
     "grade": false,
     "grade_id": "cell-edbc44e7eed6dd74",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from emoji import UNICODE_EMOJI\n",
    "\n",
    "def extract_emojis(text):\n",
    "    \"\"\"\n",
    "    Extract all emojis from a str\n",
    "    \"\"\"\n",
    "    return [ch for ch in text if ch in UNICODE_EMOJI['en']]\n",
    "\n",
    "class TwitterStream:\n",
    "    \"\"\"\n",
    "    Used to simulate a Twitter stream. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_file):\n",
    "        self.data_file = data_file\n",
    "        self.data = open(self.data_file, \"r\")\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self.reset()\n",
    "    \n",
    "    def __next__(self):\n",
    "        next_line = self.data.readline()\n",
    "        if next_line:\n",
    "            return json.loads(next_line)[\"text\"]\n",
    "        else:\n",
    "            raise StopIteration\n",
    "    \n",
    "    def __del__(self):\n",
    "        if not self.data.closed:\n",
    "            self.data.close()\n",
    "    \n",
    "    def reset(self):\n",
    "        if not self.data.closed:\n",
    "            self.data.close()\n",
    "        self.data = open(self.data_file, \"r\")\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e11b2e2ae24ed5ee1f006a98c9d66a90",
     "grade": false,
     "grade_id": "cell-ffd221b6dbe6c445",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Above we have imported the same `TwitterStream` class defined in Part 1 to simulate a Twitter stream. Remember, we are still facing one of the biggest challenges in mining data streams, that **we have limited storage capacity for the very high volume of incoming data, which may arrive at a very high velocity as well**. However, if we are only interested in the distribution of some simple items, such as emojis in this case, it might be possible to obtain approximate counts directly without curating a sample like what we did in Part 1. So let's now start exploring that possibility. \n",
    "\n",
    "\n",
    "Again, there's a helper function `extract_emojis` available that helps you extract all emojis from a piece of text, and the variable `UNICODE_EMOJI` is a collection of all emojis that are circulating around the world. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a9ca1372f3120ac11b687f39a2d7e663",
     "grade": false,
     "grade_id": "cell-d63bebc6fef0f0d1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 1: Bloom Filters (25 pts)\n",
    "\n",
    "Recall from the lectures that a Bloom filter doesn't really count items in a data stream but is able to tell\n",
    "* that an item has *definitely not appeared* in the data stream so far; or\n",
    "\n",
    "\n",
    "* that an item has *possibly appeared* in the data stream so far. \n",
    "\n",
    "In this question, we'll implement a Bloom filter for emojis in a Twitter stream. \n",
    "\n",
    "A partially completed `BloomFilter` class is given to you below. It already has the two key ingradients of a Bloom filter: a number of `slots` to record the appearance of an item and a collection, `hash_fns`, of hash functions to compute the fingerprint of an item. Your job is to complete the following two functions:\n",
    "\n",
    "* `check_appearance`: it receives a single item and returns a `bool` value indicating whether the item has appeared or not so far;\n",
    "\n",
    "\n",
    "* `do_filtering`: it receives a stream object and iterates over the stream. During each iteration, it extracts all emojis from a tweet, computes the fingerprint of each emoji and records the appearance of each emoji accordingly, as specified in the lecture slides. Finally, it returns a copy of the `slots` of your `BloomFilter` for grading at every iteration, which you don't need to worry about. **However, please do make sure that you don't inadvertently change the indentation of the `yield` statement.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2d8cbeecce638a152b3763c16d11bc57",
     "grade": false,
     "grade_id": "cell-580c8a3fe199c750",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "There is also an accompanying `HashFunction` class that provides simple and deterministic hash functions. Once instantiated, they behave just like ordinary Python functions. For example, the code below computes the fingerprint of ðŸ˜‚, assuming we have `7919` (the 1000-th prime number) slots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a461d6082d45a9b74bf446ed5884934",
     "grade": false,
     "grade_id": "cell-6e0fae93cd740963",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class HashFunction:\n",
    "    def __init__(self, num_slots):\n",
    "        self.num_slots = num_slots\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return (hash(self) + hash(x)) % self.num_slots\n",
    "\n",
    "h1, h2 = HashFunction(7919), HashFunction(7919)\n",
    "\n",
    "# The two hash functions are distinct, but both are deterministic\n",
    "print(h1(\"ðŸ˜‚\"), h2(\"ðŸ˜‚\"))\n",
    "print(h1(\"ðŸ˜‚\"), h2(\"ðŸ˜‚\"))\n",
    "\n",
    "del h1, h2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d0df67b6e519fdbf94e182af45582568",
     "grade": false,
     "grade_id": "cell-9245e216080b9e5f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "It's worth noting that two different instantiations of the `HashFunction` class lead to two distinct hash functions, in that they assign different fingerprints to the same emoji. However, they are both deterministic, in that they always assign the same fingerprint to an emoji regardless of how many times you apply them. Every time you re-run the code above, the two hash functions will change and so will the fingerprints, but they will always be deterministic. These two properties may have some implications on your debugging strategies later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "672436d6c6c50457c1b82256ba5eda4a",
     "grade": false,
     "grade_id": "cell-993d3b939f534f62",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BloomFilter:\n",
    "    \n",
    "    def __init__(self, num_slots, num_hash_fns):\n",
    "        \n",
    "        self.slots = np.zeros(num_slots, dtype=int)\n",
    "        self.hash_fns = [HashFunction(num_slots) for _ in range(num_hash_fns)] # A list of distinct hash functions\n",
    "    \n",
    "    def check_appearance(self, item):\n",
    "        \"\"\"\n",
    "        Returns a bool value indicating whether an item has appeared or not\n",
    "        \"\"\"\n",
    "        \n",
    "        has_appeared = None\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        return has_appeared\n",
    "    \n",
    "    def do_filtering(self, stream):\n",
    "        \"\"\"\n",
    "        Iterates over a stream, collects items of interest, calculates the fingerprints and records the appearance\n",
    "        \"\"\"\n",
    "        \n",
    "        self.slots = np.zeros_like(self.slots) # reset the slots\n",
    "        \n",
    "        for item in stream: # iterate over the stream\n",
    "            \n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "            # returns a copy of slots at the end of every iteration for grading - code given\n",
    "            yield self.slots.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f961d1fda62a6b1beee63c581de2029d",
     "grade": true,
     "grade_id": "cell-a7b7858dc829b35f",
     "locked": true,
     "points": 25,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder tests\n",
    "\n",
    "from emoji import UNICODE_EMOJI\n",
    "\n",
    "twitter_stream = TwitterStream(\"assets/tweets\")\n",
    "\n",
    "num_slots, num_hash_fns = 7919, 5\n",
    "stu_ans = BloomFilter(num_slots, num_hash_fns)\n",
    "\n",
    "# Collect emojis that appeared and that didn't appear\n",
    "emojis_appeared = set()\n",
    "for tweet in twitter_stream:\n",
    "    emojis_appeared = emojis_appeared.union(extract_emojis(tweet))\n",
    "emojis_not_appeared = set(UNICODE_EMOJI.keys()) - emojis_appeared\n",
    "\n",
    "# Do filtering. Don't have to collect the results. Just exhaust the stream\n",
    "for _ in stu_ans.do_filtering(twitter_stream):\n",
    "    pass\n",
    "\n",
    "\n",
    "# Check that the check_appearance function returns a bool\n",
    "assert isinstance(stu_ans.check_appearance(\"ðŸ˜‚\"), (bool, np.bool_)), \"Q1: Your check_appearance function should return a bool value. \"\n",
    "\n",
    "\n",
    "# Check that every item that appeared should be marked as appeared - correctness\n",
    "for emoji in emojis_appeared:\n",
    "    assert stu_ans.check_appearance(emoji), f\"Q1: {emoji} appeared but is marked as not appeared. \"\n",
    "\n",
    "    \n",
    "# Check that every item that is marked as not appeared really didn't appear - no false negatives\n",
    "for emoji in UNICODE_EMOJI:\n",
    "    if not stu_ans.check_appearance(emoji):\n",
    "        assert emoji in emojis_not_appeared, f\"Q1: {emoji} marked as not appeared but actually appeared. \"\n",
    "\n",
    "\n",
    "# Start a new filtering for the hidden tests\n",
    "stu_slots = stu_ans.do_filtering(twitter_stream)\n",
    "\n",
    "# Some hidden tests\n",
    "\n",
    "\n",
    "del num_slots, num_hash_fns, twitter_stream, stu_ans, stu_slots, emojis_appeared, emojis_not_appeared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "248fe7890331d2f941478422199a5223",
     "grade": false,
     "grade_id": "cell-2939c5cace465fa0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 2: Lossy Counter (25 pts)\n",
    "\n",
    "With reference to the lecture slides, let's now implement a lossy counter for emojis. The lossy counter should maintain counts of all emojis seen so far and only update the counts once a \"bucket\" of tweets arrive. The \"update\" of counts should include increments due to the emojis contained in the new bucket and decrements because we want to gradually get rid of less recent emojis. \n",
    "\n",
    "Again, a partially completed `LossyCounter` class is given to you below. Your job is to complete the `do_counting` function. It receives a stream object and iterates over the stream. Once a bucket of tweets have fully arrived, it updates the emoji counts as specified in the lecture slides. It returns a copy of the `counts` of your `LossyCounter` for grading at every iteration, which you don't need to worry about. **However, please do make sure that you don't inadvertently change the indentation of the `yield` statement and that there is always a `yield` statement being executed *at every iteration*.**\n",
    "\n",
    "A few notes on implementation:\n",
    "\n",
    "* The autograder expects that all the requisite updates to emoji counts, **including both increments and decrements**, have been performed when it starts to check your `self.counts` for grading, **immediately after a full bucket of tweets have arrived**. For example, if `self.bucket_size == 5`, the autograder will examine the content of your `self.counts` for grading right after the fifth tweet has been consumed by your `LossyCounter`; \n",
    "\n",
    "\n",
    "* When your `LossyCounter` is dropping an emoji, it's not enough to set the count of that emoji to zero. The emoji must be completely deleted from your counts, as if it never appeared (why?);  \n",
    "\n",
    "\n",
    "* You have complete freedom in how you'd like to implement the \"bucket\". In fact, not being a sampling algorithm, your `LossyCounter` doesn't have to actually store tweets in a bucket. You only need to make sure the emoji counts are updated correctly when a full bucket of tweets have arrived, since that's all what the autograder checks. \n",
    "\n",
    "\n",
    "* In the extreme case where the bucket size is strictly greater than the total number of tweets in the stream, your `LossyCounter` should not be lossy anymore, that is, we won't do decrements but only increments, since we would never see a full bucket arriving. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f935c842a52fb6d96fbbfe40606f1f61",
     "grade": false,
     "grade_id": "cell-e896207bcc59798b",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class LossyCounter:\n",
    "    \n",
    "    def __init__(self, bucket_size):\n",
    "        \n",
    "        self.bucket_size = bucket_size\n",
    "        self.counts = defaultdict(int) # recommended to use defaultdict, but an ordinary dict works fine too\n",
    "    \n",
    "    def do_counting(self, stream):\n",
    "        \"\"\"\n",
    "        Iterates over a stream, counts the items and drops the infrequent ones in a bucket\n",
    "        \"\"\"\n",
    "        \n",
    "        self.counts.clear() # reset the counts\n",
    "        num_items_in_bucket = 0 # optional: the current number of items in the \"bucket\"\n",
    "                \n",
    "        for item in stream: # iterate over the stream\n",
    "            \n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "            # returns a copy of counts at the end of every iteration for grading - code given\n",
    "            yield self.counts.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6ad2825d3a2f074656de47b99ba112ac",
     "grade": true,
     "grade_id": "cell-b9358e200e7341eb",
     "locked": true,
     "points": 25,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder tests\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "twitter_stream = TwitterStream(\"assets/tweets\")\n",
    "\n",
    "# Sanity checks for a trivial case - use a large bucket size to include all tweets\n",
    "bucket_size = 100000\n",
    "stu_ans = LossyCounter(bucket_size)\n",
    "\n",
    "# Collect all emojis that appeared\n",
    "emojis_appeared = set()\n",
    "for tweet in twitter_stream:\n",
    "    emojis_appeared = emojis_appeared.union(extract_emojis(tweet))\n",
    "\n",
    "# Do counting. Don't have to collect the results. Just exhaust the stream\n",
    "for _ in stu_ans.do_counting(twitter_stream):\n",
    "    pass\n",
    "\n",
    "\n",
    "assert isinstance(stu_ans.counts, dict), \"Q2: You should store counts in a dict. \"\n",
    "\n",
    "assert len(stu_ans.counts) == len(emojis_appeared), f\"Q2: The length of your emoji counts ({len(stu_ans.counts)}) differs from the correct answer ({len(emojis_appeared)}). \"\n",
    "\n",
    "assert not (emojis_appeared - set(stu_ans.counts.keys())), f\"Q2: Your emoji counts don't include {emojis_appeared - set(stu_ans.counts.keys())}. \"\n",
    "\n",
    "assert not (set(stu_ans.counts.keys()) - emojis_appeared), f\"Q2: Your emoji counts contain extra emojis: {set(stu_ans.counts.keys()) - emojis_appeared}. \"\n",
    "\n",
    "\n",
    "# Re-define variables for the hidden tests\n",
    "bucket_size = 100\n",
    "stu_ans = LossyCounter(bucket_size)\n",
    "stu_counts = stu_ans.do_counting(twitter_stream)\n",
    "\n",
    "# Some hidden tests\n",
    "\n",
    "\n",
    "del twitter_stream, stu_ans, stu_counts, emojis_appeared, bucket_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "be0cc13f5017b3677e32c7951160f8b5",
     "grade": false,
     "grade_id": "cell-b06194f2c905a730",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Let's see what the emoji distribution is after all tweets are processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_size = 100\n",
    "stu_ans = LossyCounter(bucket_size)\n",
    "\n",
    "# Do counting. Don't have to collect the results. Just exhaust the stream\n",
    "for _ in stu_ans.do_counting(TwitterStream(\"assets/tweets\")):\n",
    "    pass\n",
    "\n",
    "sorted_counts = {emoji: stu_ans.counts[emoji] for emoji in sorted(stu_ans.counts.keys(), key=stu_ans.counts.get, reverse=True)}\n",
    "print(sorted_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "30814d9953a9ee6359e69bc0693a4a4f",
     "grade": false,
     "grade_id": "cell-5246f448e422cd04",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Visualised in a bar graph, the emoji distribution seems to resemble a [Power Law](https://en.wikipedia.org/wiki/Power_law) distribution. A few emojis are used a lot while the majority of the emojis are rarely used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.bar(range(len(sorted_counts)), sorted_counts.values())\n",
    "ax.set_xlabel(\"Rank\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.set_title(\"Emoji Distribution\")\n",
    "\n",
    "del fig, ax"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
